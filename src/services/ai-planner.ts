/**
 * AI Planner Service
 * Intelligently generates workflow plans based on user input
 */

import { getFalAdapters, DEFAULT_MODELS } from "@/integrations/ai-adapters"
import { DeepSeekLLMAdapter } from "@/integrations/ai-adapters/deepseek"
import { estimateWorkflowCost } from "./pricing"
import { createWorkflowBuilder } from "./workflow-builder"

// ============ Type Definitions ============

export interface UserInput {
  prompt: string // User's video description
  duration: number // Video length in seconds (8, 15, 30, or 60)
  aspectRatio: string // "9:16", "16:9", or "1:1"
  voice?: string // "none", "male", "female", "clone"
  referenceImage?: string // URL or null
  style?: string // "photorealistic", "anime", "cartoon", etc.
}

export interface Shot {
  second: number // Which second this shot represents (0-based)
  shotType: string // "ECU" (extreme close-up), "CU" (close-up), "MS" (medium shot), "WS" (wide shot), "EWS" (extreme wide shot)
  action: string // What's happening in this specific second
  cameraMove?: string // "push in", "pull out", "pan left/right", "tilt up/down", "static"
}

export interface Scene {
  id: string
  duration: number
  description: string // Overall scene description for video generation
  shots?: Shot[] // Second-by-second shot breakdown (optional, for advanced planning)
  cameraAngle?: string // Overall camera angle for the scene
  movement?: string // Overall camera movement for the scene
  audio?: string // Sound effects, ambience, music cues
  dialogue?: string | null // Character dialogue if applicable
  stylePreset?: string
}

export interface VoiceoverPlan {
  script: string
  voice: string
  language: string
  estimatedDuration: number
}

export interface WorkflowPlan {
  scenes: Scene[]
  voiceover?: VoiceoverPlan
  music?: {
    mood: string
    genre: string
  }
  aspectRatio: string
  recommendedModels: {
    llm: string
    t2i: string
    t2v: string
    tts?: string
  }
  estimatedCredits: number
  // Dynamic workflow nodes (generated by WorkflowBuilder)
  workflowNodes?: any[]  // WorkflowNode[] from workflow-builder.ts
}

// ============ AI Planner Implementation ============

export class AIPlanner {
  private adapters = getFalAdapters()
  private deepseekLLM = new DeepSeekLLMAdapter()

  /**
   * Generate a complete workflow plan from user input
   */
  async generateWorkflow(input: UserInput): Promise<WorkflowPlan> {
    try {
      // Step 0: Auto-detect aspect ratio if needed
      const detectedAspectRatio = this.detectAspectRatio(input)
      const finalInput = {
        ...input,
        aspectRatio: detectedAspectRatio
      }

      // Step 1: Use LLM to analyze and create structured plan
      const plannerPrompt = this.buildPlannerPrompt(finalInput)

      // Use DeepSeek for LLM (not Fal.ai)
      const llmResponse = await this.deepseekLLM.call({
        model: DEFAULT_MODELS.llm,
        prompt: plannerPrompt.userPrompt,
        systemPrompt: plannerPrompt.systemPrompt,
        temperature: 0.7,
        maxTokens: 2000,
        responseFormat: "json",
      })

      const planData = JSON.parse(llmResponse.output)

      // Step 2: Build workflow plan
      const workflow: WorkflowPlan = {
        scenes: planData.scenes.map((scene: any, index: number) => ({
          id: `scene_${index + 1}`,
          duration: scene.duration,
          description: scene.description,
          shots: scene.shots || undefined, // Second-by-second shot breakdown (optional)
          cameraAngle: scene.cameraAngle || "mid-shot",
          movement: scene.movement || "push in", // Default to "push in" for more dynamic videos
          audio: scene.audio || undefined,
          dialogue: scene.dialogue || null,
          stylePreset: input.style || "photorealistic",
        })),
        aspectRatio: detectedAspectRatio,
        recommendedModels: {
          llm: DEFAULT_MODELS.llm,
          t2i: this.selectBestT2IModel(input),
          t2v: this.selectBestT2VModel(input),
        },
        estimatedCredits: 0,
      }

      // Step 3: Add voiceover plan if requested
      if (input.voice && input.voice !== "none") {
        workflow.voiceover = {
          script: planData.voiceoverScript || "",
          voice: input.voice,
          language: "en",
          estimatedDuration: input.duration,
        }
        workflow.recommendedModels.tts = DEFAULT_MODELS.tts
      }

      // Step 4: Add music suggestion if available
      if (planData.music) {
        workflow.music = planData.music
      }

      // Step 5: Calculate estimated credits
      workflow.estimatedCredits = this.calculateEstimatedCredits(workflow)

      // Step 6: Generate workflow nodes using WorkflowBuilder
      const workflowBuilder = createWorkflowBuilder()
      workflow.workflowNodes = workflowBuilder.buildWorkflow(
        workflow.scenes,
        workflow.voiceover,
        {
          t2i: workflow.recommendedModels.t2i,
          i2v: workflow.recommendedModels.t2v,
          tts: workflow.recommendedModels.tts
        },
        {
          aspectRatio: detectedAspectRatio,
          referenceImage: finalInput.referenceImage
        }
      )

      return workflow
    } catch (error: any) {
      console.error("AI Planner error:", error)
      throw new Error(`Failed to generate workflow: ${error.message}`)
    }
  }

  /**
   * Get human-readable aspect ratio description
   */
  private getAspectRatioDescription(aspectRatio: string): string {
    switch (aspectRatio) {
      case '9:16':
        return 'TikTok/Reels vertical format - mobile-first, portrait orientation';
      case '16:9':
        return 'YouTube/Bilibili landscape format - widescreen, desktop-first';
      case '1:1':
        return 'Instagram square format - equal width and height';
      case '4:3':
        return 'Traditional video format - slightly taller than wide';
      case '21:9':
        return 'Cinematic ultra-wide format - dramatic widescreen';
      default:
        return 'Standard video format';
    }
  }

  /**
   * Auto-detect aspect ratio based on user prompt keywords
   * TikTok/抖音/Reels/Stories → 9:16 (vertical)
   * YouTube/横屏/landscape → 16:9 (horizontal)
   * Instagram/正方形 → 1:1 (square)
   */
  private detectAspectRatio(input: UserInput): string {
    const prompt = input.prompt.toLowerCase()

    // Vertical video keywords (9:16)
    const verticalKeywords = [
      'tiktok', 'tik tok', '抖音', '快手', 'kuaishou',
      'reels', 'instagram reels', 'ig reels',
      'stories', 'instagram stories', 'ig stories',
      'shorts', 'youtube shorts',
      '竖屏', '竖版', 'vertical', 'portrait'
    ]

    // Horizontal video keywords (16:9)
    const horizontalKeywords = [
      'youtube', 'bilibili', 'b站',
      '横屏', '横版', 'landscape', 'horizontal',
      'vlog', '教程', 'tutorial', 'review', '评测'
    ]

    // Square video keywords (1:1)
    const squareKeywords = [
      'instagram post', 'ig post',
      '正方形', 'square',
      'facebook post'
    ]

    // Check vertical keywords first (highest priority for mobile-first content)
    for (const keyword of verticalKeywords) {
      if (prompt.includes(keyword)) {
        console.log(`[AI Planner] Detected vertical video from keyword: "${keyword}"`)
        return '9:16'
      }
    }

    // Check square keywords
    for (const keyword of squareKeywords) {
      if (prompt.includes(keyword)) {
        console.log(`[AI Planner] Detected square video from keyword: "${keyword}"`)
        return '1:1'
      }
    }

    // Check horizontal keywords
    for (const keyword of horizontalKeywords) {
      if (prompt.includes(keyword)) {
        console.log(`[AI Planner] Detected horizontal video from keyword: "${keyword}"`)
        return '16:9'
      }
    }

    // If user provided aspect ratio, use it
    if (input.aspectRatio && ['9:16', '16:9', '1:1'].includes(input.aspectRatio)) {
      console.log(`[AI Planner] Using user-provided aspect ratio: ${input.aspectRatio}`)
      return input.aspectRatio
    }

    // Default to vertical (9:16) for social media era
    console.log('[AI Planner] No keywords detected, defaulting to vertical 9:16')
    return '9:16'
  }

  /**
   * Calculate recommended scene count based on total duration
   * 8s → 1 scene, 15s → 2 scenes, 30s → 3 scenes, 60s → 5-6 scenes
   */
  private getRecommendedSceneCount(duration: number): { min: number; max: number } {
    if (duration <= 8) return { min: 1, max: 1 }
    if (duration <= 15) return { min: 2, max: 2 }
    if (duration <= 30) return { min: 3, max: 3 }
    // For 60s, use 5-6 scenes to leverage Sora 2's 12s max capability
    return { min: 5, max: 6 }
  }

  /**
   * Build the prompt for the LLM planner
   */
  private buildPlannerPrompt(input: UserInput) {
    const sceneCount = this.getRecommendedSceneCount(input.duration)
    const sceneGuidance = sceneCount.min === sceneCount.max
      ? `exactly ${sceneCount.min} scene${sceneCount.min > 1 ? 's' : ''}`
      : `${sceneCount.min}-${sceneCount.max} scenes`

    const systemPrompt = `You are an expert film director and cinematographer. Create a detailed, director-level storyboard with ${sceneGuidance} for a ${input.duration}-second video.

**Scene Requirements** (create ${sceneGuidance}):

1. **Duration**: Each scene duration must be specified. Total must sum to EXACTLY ${input.duration} seconds.
   - For ${input.duration}s video: Use ${sceneGuidance}, with each scene 8-12 seconds (optimal for Sora 2 model)

2. **Visual Description**: Write cinematic, detailed descriptions suitable for AI video generation. **CRITICAL REQUIREMENTS**:
   - **Include second-by-second time markers**: [0-2s], [2-4s], [4-7s], etc.
   - **Specify aspect ratio**: "${input.aspectRatio}" (${this.getAspectRatioDescription(input.aspectRatio)})
   - Subject and action (what's happening at each second)
   - Shot type progression (Wide shot → Medium shot → Close-up)
   - Camera movement (push in, pan left, static, etc.)
   - Composition and framing (rule of thirds, symmetry, depth)
   - Lighting (golden hour, dramatic shadows, soft key light, etc.)
   - Color palette and mood
   - Environmental details (background, atmosphere, weather)

3. **Shot Type Progression** (景别切换 - CRITICAL for visual flow):
   - Use professional shot types: ECU (extreme close-up), CU (close-up), MS (medium shot), WS (wide shot), EWS (extreme wide shot)
   - **Scene 1**: Start with WS or MS to establish context
   - **Scene 2**: Transition to CU or MS for emotional connection
   - **Scene 3+**: Alternate between shot types for visual variety
   - **AVOID**: Using the same shot type consecutively across scenes

4. **Second-by-Second Breakdown** (OPTIONAL but RECOMMENDED for precise control):
   - Break each scene into second-by-second shots
   - Specify shot type and action for each second
   - This enables precise camera movements and visual transitions
   - Example: [{"second": 0, "shotType": "WS", "action": "Car enters frame from left", "cameraMove": "static"}]

5. **Camera Movement** (use professional terminology):
   - Static: Fixed camera, no movement
   - Push in / Pull out: Smooth forward/backward movement (preferred over dolly for AI)
   - Pan left/right: Horizontal camera rotation
   - Tilt up/down: Vertical camera rotation
   - Zoom-in / Zoom-out: Lens focal length change
   - Tracking: Camera follows subject
   - Crane: Vertical movement with sweeping motion

6. **Audio Description** (sound effects, ambience, music cues):
   - Describe background sounds (city ambience, nature sounds, etc.)
   - Sound effects (footsteps, door closing, glass clinking, etc.)
   - Music mood/style if applicable

7. **Dialogue** (if characters are speaking):
   - Include exact dialogue in quotes
   - Describe delivery style (whispered, shouted, emotional, etc.)

${input.voice && input.voice !== "none" ? `
**Voiceover Script**: Create a compelling ${input.voice} voiceover that:
- Matches the ${input.duration}-second duration (approximately ${Math.floor(input.duration * 2.5)} words)
- Enhances the visual narrative
- Has natural pacing with pauses for impact
` : ""}

**Output Format** (valid JSON only):
{
  "scenes": [
    {
      "duration": 8,
      "description": "[0-2s] Wide shot. A sleek Tesla Model 3 enters frame from left, driving along Pacific Coast Highway at golden hour. [2-4s] Camera slowly pushes in as the car approaches center frame. Ocean waves visible in background. [4-6s] Transition to medium shot, car's metallic blue paint gleams in warm sunlight. [6-8s] Hold medium shot, sunlight reflects off hood. Cinematic color grading with teal shadows and orange highlights. 16:9 widescreen format. Camera movement: static → push in → push in → static.",
      "cameraAngle": "wide",
      "movement": "push in",
      "audio": "Ocean waves, wind rushing, electric motor hum",
      "dialogue": null,
      "shots": [
        {"second": 0, "shotType": "WS", "action": "Car enters frame from left on coastal highway", "cameraMove": "static"},
        {"second": 2, "shotType": "WS", "action": "Car driving center frame, ocean visible", "cameraMove": "push in"},
        {"second": 4, "shotType": "MS", "action": "Car closer now, details visible", "cameraMove": "push in"},
        {"second": 6, "shotType": "MS", "action": "Car fills frame, sunlight reflects off hood", "cameraMove": "static"}
      ]
    },
    {
      "duration": 7,
      "description": "[0-2s] Close-up. Hands grip steering wheel, camera static. [2-4s] Finger taps touchscreen, dashboard lights glow. [4-5s] Camera pans right to show wood grain accents. [5-7s] Hands adjust gear selector. Modern minimalist interior with touchscreen dashboard displaying navigation. Soft natural light from sunroof creates gentle shadows. Static framing emphasizes tactile details and premium materials. 16:9 format. Camera: static → static → pan right → static.",
      "cameraAngle": "close-up",
      "movement": "static",
      "audio": "Soft electronic beep from dashboard, leather seats creaking",
      "dialogue": null,
      "shots": [
        {"second": 0, "shotType": "CU", "action": "Hands grip steering wheel", "cameraMove": "static"},
        {"second": 2, "shotType": "CU", "action": "Finger taps touchscreen", "cameraMove": "static"},
        {"second": 4, "shotType": "CU", "action": "Dashboard lights glow", "cameraMove": "pan right"},
        {"second": 6, "shotType": "CU", "action": "Hands adjust gear selector", "cameraMove": "static"}
      ]
    }
  ],
  "voiceoverScript": "Experience the future of driving. Where cutting-edge technology meets timeless design.",
  "music": {
    "mood": "inspiring and modern",
    "genre": "electronic ambient"
  }
}`

    const userPrompt = `Create a cinematic ${input.duration}-second video storyboard for:

**User Brief**: "${input.prompt}"

**Production Specs**:
- Total Duration: ${input.duration} seconds (MUST be exact)
- Scene Count: ${sceneGuidance}
- Aspect Ratio: ${input.aspectRatio}
- Visual Style: ${input.style || "photorealistic"}
${input.voice && input.voice !== "none" ? `- Voiceover: ${input.voice} voice (${Math.floor(input.duration * 2.5)} words approx.)` : "- No voiceover (rely on visuals and ambient audio)"}
${input.referenceImage ? `- Reference Image: Provided (match visual style, color palette, and mood)` : ""}

**Director's Notes** (CRITICAL - MUST FOLLOW):
✓ Scene durations MUST sum to EXACTLY ${input.duration} seconds
✓ **REQUIRED**: Include time markers in description: [0-2s], [2-4s], [4-7s], etc.
✓ **REQUIRED**: Mention aspect ratio in description: "${input.aspectRatio}" (${this.getAspectRatioDescription(input.aspectRatio)})
✓ **REQUIRED**: Describe what happens at each 2-second interval
✓ **REQUIRED**: Specify camera movement changes (static → push in → pan left, etc.)
✓ Use professional camera movements (push in, pull out, pan, tracking, etc.) - vary movements for visual interest
✓ Each scene needs cinematic visual descriptions (lighting, composition, color grading)
✓ Include second-by-second shot breakdowns in the "shots" array (recommended for precise control)
✓ Use shot type progression (WS → MS → CU) to create visual flow between scenes
✓ Include audio descriptions (ambience, sound effects, music cues)
✓ Add dialogue if characters are speaking in the scene
✓ Ensure narrative flow and visual variety between scenes
✓ Leverage the 8-12 second "sweet spot" for Sora 2 video generation
${input.voice && input.voice !== "none" ? `✓ Voiceover script should complement visuals, not just describe them` : ""}
${input.referenceImage ? `
✓ **CHARACTER CONSISTENCY** (CRITICAL):
  - When describing characters/subjects across scenes, use "SAME [character]" format
  - Example: "SAME woman from previous scene now sitting at desk"
  - Example: "SAME blue Tesla car from opening shot"
  - This ensures the AI maintains visual consistency across all scenes
  - Apply same lighting, clothing, and visual style as the reference image
` : ""}

Output ONLY valid JSON (no markdown, no additional text).`

    return { systemPrompt, userPrompt }
  }

  /**
   * Select the best T2I model based on input characteristics
   */
  private selectBestT2IModel(input: UserInput): string {
    // For cartoon/anime styles, use NanoBanana
    if (input.style === "cartoon" || input.style === "anime") {
      return "fal-ai/nanobanana"
    }

    // Default to FLUX-dev for best quality/cost balance
    return DEFAULT_MODELS.t2i
  }

  /**
   * Select the best T2V model based on input characteristics
   * Considers: reference image, audio needs, scene count, budget
   */
  private selectBestT2VModel(input: UserInput): string {
    const sceneCount = this.getRecommendedSceneCount(input.duration)
    const needsMultipleScenes = sceneCount.min > 1
    const hasReferenceImage = !!input.referenceImage
    const needsAudio = input.voice && input.voice !== "none"

    // Decision Tree (Updated for cost efficiency):

    // 1. Multi-scene videos (15s+) WITHOUT reference image → Kling v1.6 Standard I2V (economical, high quality)
    if (needsMultipleScenes && !hasReferenceImage) {
      console.log('[AI Planner] Multi-scene T2V → Kling v1.6 Standard I2V (economical, high quality)')
      return "fal-ai/kling-video/v1.6/standard/image-to-video"
    }

    // 2. Multi-scene videos with reference image → Kling v1.6 Standard I2V (good continuity, economical)
    if (needsMultipleScenes && hasReferenceImage) {
      console.log('[AI Planner] Multi-scene + reference image → Kling v1.6 Standard I2V')
      return "fal-ai/kling-video/v1.6/standard/image-to-video"
    }

    // 3. Single scene with reference image → Kling v1.6 or Seedance I2V
    if (!needsMultipleScenes && hasReferenceImage) {
      // For higher quality, use Kling v1.6 I2V
      if (needsAudio) {
        console.log('[AI Planner] Single scene I2V + audio → Kling v1.6 I2V')
        return "fal-ai/kling-video/v1.6/standard/image-to-video"
      }
      // Budget-friendly option: Seedance I2V (ultra-cheap)
      console.log('[AI Planner] Single scene I2V budget → Seedance I2V')
      return "fal-ai/seedance/image-to-video"
    }

    // 4. Single scene WITHOUT reference image (pure T2V)
    if (!needsMultipleScenes && !hasReferenceImage) {
      // Budget-friendly: Seedance T2V (for short clips)
      console.log('[AI Planner] Single scene T2V budget → Seedance T2V')
      return "fal-ai/seedance/text-to-video"
    }

    // Fallback: Kling v1.6 Standard I2V (economical, reliable)
    console.log('[AI Planner] Fallback → Kling v1.6 Standard I2V')
    return "fal-ai/kling-video/v1.6/standard/image-to-video"
  }

  /**
   * Calculate estimated credits for the workflow
   * Uses shared pricing module to ensure estimate = deduction parity
   */
  private calculateEstimatedCredits(workflow: WorkflowPlan): number {
    const totalDuration = workflow.scenes.reduce((sum, s) => sum + s.duration, 0)

    const estimate = estimateWorkflowCost({
      sceneCount: workflow.scenes.length,
      totalDurationSeconds: totalDuration,
      hasVoiceover: !!workflow.voiceover,
      t2iModel: workflow.recommendedModels.t2i,
      t2vModel: workflow.recommendedModels.t2v,
      ttsModel: workflow.recommendedModels.tts,
    })

    // Round up to nearest 0.5 for user-facing display
    return Math.ceil(estimate * 2) / 2
  }
}

/**
 * Helper function to create AI Planner instance
 */
export function createAIPlanner() {
  return new AIPlanner()
}
