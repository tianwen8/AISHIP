/**
 * AI Planner Service
 * Intelligently generates workflow plans based on user input
 */

import { getFalAdapters, DEFAULT_MODELS } from "@/integrations/ai-adapters"
import { DeepSeekLLMAdapter } from "@/integrations/ai-adapters/deepseek"
import { estimateWorkflowCost } from "./pricing"
import { createWorkflowBuilder } from "./workflow-builder"

// ============ Type Definitions ============

export interface UserInput {
  prompt: string // User's video description
  duration: number // Video length in seconds (8, 15, 30, or 60)
  aspectRatio: string // "9:16", "16:9", or "1:1"
  voice?: string // "none", "male", "female", "clone"
  referenceImage?: string // URL or null
  style?: string // "photorealistic", "anime", "cartoon", etc.
}

export interface Scene {
  id: string
  duration: number
  description: string // Detailed cinematic prompt for video generation
  cameraAngle?: string // "close-up", "mid-shot", "wide", "aerial", etc.
  movement?: string // "static", "dolly-in", "pan-left", "tracking", etc.
  audio?: string // Sound effects, ambience, music cues
  dialogue?: string | null // Character dialogue if applicable
  stylePreset?: string
}

export interface VoiceoverPlan {
  script: string
  voice: string
  language: string
  estimatedDuration: number
}

export interface WorkflowPlan {
  scenes: Scene[]
  voiceover?: VoiceoverPlan
  music?: {
    mood: string
    genre: string
  }
  aspectRatio: string
  recommendedModels: {
    llm: string
    t2i: string
    t2v: string
    tts?: string
  }
  estimatedCredits: number
  // Dynamic workflow nodes (generated by WorkflowBuilder)
  workflowNodes?: any[]  // WorkflowNode[] from workflow-builder.ts
}

// ============ AI Planner Implementation ============

export class AIPlanner {
  private adapters = getFalAdapters()
  private deepseekLLM = new DeepSeekLLMAdapter()

  /**
   * Generate a complete workflow plan from user input
   */
  async generateWorkflow(input: UserInput): Promise<WorkflowPlan> {
    try {
      // Step 1: Use LLM to analyze and create structured plan
      const plannerPrompt = this.buildPlannerPrompt(input)

      // Use DeepSeek for LLM (not Fal.ai)
      const llmResponse = await this.deepseekLLM.call({
        model: DEFAULT_MODELS.llm,
        prompt: plannerPrompt.userPrompt,
        systemPrompt: plannerPrompt.systemPrompt,
        temperature: 0.7,
        maxTokens: 2000,
        responseFormat: "json",
      })

      const planData = JSON.parse(llmResponse.output)

      // Step 2: Build workflow plan
      const workflow: WorkflowPlan = {
        scenes: planData.scenes.map((scene: any, index: number) => ({
          id: `scene_${index + 1}`,
          duration: scene.duration,
          description: scene.description,
          cameraAngle: scene.cameraAngle || "mid-shot",
          movement: scene.movement || "dolly-in", // Default to dolly-in for more dynamic videos
          audio: scene.audio || undefined,
          dialogue: scene.dialogue || null,
          stylePreset: input.style || "photorealistic",
        })),
        aspectRatio: input.aspectRatio,
        recommendedModels: {
          llm: DEFAULT_MODELS.llm,
          t2i: this.selectBestT2IModel(input),
          t2v: this.selectBestT2VModel(input),
        },
        estimatedCredits: 0,
      }

      // Step 3: Add voiceover plan if requested
      if (input.voice && input.voice !== "none") {
        workflow.voiceover = {
          script: planData.voiceoverScript || "",
          voice: input.voice,
          language: "en",
          estimatedDuration: input.duration,
        }
        workflow.recommendedModels.tts = DEFAULT_MODELS.tts
      }

      // Step 4: Add music suggestion if available
      if (planData.music) {
        workflow.music = planData.music
      }

      // Step 5: Calculate estimated credits
      workflow.estimatedCredits = this.calculateEstimatedCredits(workflow)

      // Step 6: Generate workflow nodes using WorkflowBuilder
      const workflowBuilder = createWorkflowBuilder()
      workflow.workflowNodes = workflowBuilder.buildWorkflow(
        workflow.scenes,
        workflow.voiceover,
        {
          t2i: workflow.recommendedModels.t2i,
          i2v: workflow.recommendedModels.t2v,
          tts: workflow.recommendedModels.tts
        }
      )

      return workflow
    } catch (error: any) {
      console.error("AI Planner error:", error)
      throw new Error(`Failed to generate workflow: ${error.message}`)
    }
  }

  /**
   * Calculate recommended scene count based on total duration
   * 8s → 1 scene, 15s → 2 scenes, 30s → 3 scenes, 60s → 5-6 scenes
   */
  private getRecommendedSceneCount(duration: number): { min: number; max: number } {
    if (duration <= 8) return { min: 1, max: 1 }
    if (duration <= 15) return { min: 2, max: 2 }
    if (duration <= 30) return { min: 3, max: 3 }
    // For 60s, use 5-6 scenes to leverage Sora 2's 12s max capability
    return { min: 5, max: 6 }
  }

  /**
   * Build the prompt for the LLM planner
   */
  private buildPlannerPrompt(input: UserInput) {
    const sceneCount = this.getRecommendedSceneCount(input.duration)
    const sceneGuidance = sceneCount.min === sceneCount.max
      ? `exactly ${sceneCount.min} scene${sceneCount.min > 1 ? 's' : ''}`
      : `${sceneCount.min}-${sceneCount.max} scenes`

    const systemPrompt = `You are an expert film director and cinematographer. Create a detailed, director-level storyboard with ${sceneGuidance} for a ${input.duration}-second video.

**Scene Requirements** (create ${sceneGuidance}):

1. **Duration**: Each scene duration must be specified. Total must sum to EXACTLY ${input.duration} seconds.
   - For ${input.duration}s video: Use ${sceneGuidance}, with each scene 8-12 seconds (optimal for Sora 2 model)

2. **Visual Description**: Write cinematic, detailed descriptions suitable for AI video generation. Include:
   - Subject and action (what's happening)
   - Composition and framing (rule of thirds, symmetry, depth)
   - Lighting (golden hour, dramatic shadows, soft key light, etc.)
   - Color palette and mood
   - Environmental details (background, atmosphere, weather)

3. **Camera Movement** (use professional terminology):
   - Static: Fixed camera, no movement
   - Dolly-in / Dolly-out: Smooth forward/backward movement
   - Pan (left/right): Horizontal camera rotation
   - Tilt (up/down): Vertical camera rotation
   - Zoom-in / Zoom-out: Lens focal length change
   - Tracking/Following: Camera follows subject
   - Crane/Jib: Vertical movement with sweeping motion
   - Handheld: Natural shake for documentary feel

4. **Audio Description** (sound effects, ambience, music cues):
   - Describe background sounds (city ambience, nature sounds, etc.)
   - Sound effects (footsteps, door closing, glass clinking, etc.)
   - Music mood/style if applicable

5. **Dialogue** (if characters are speaking):
   - Include exact dialogue in quotes
   - Describe delivery style (whispered, shouted, emotional, etc.)

${input.voice && input.voice !== "none" ? `
**Voiceover Script**: Create a compelling ${input.voice} voiceover that:
- Matches the ${input.duration}-second duration (approximately ${Math.floor(input.duration * 2.5)} words)
- Enhances the visual narrative
- Has natural pacing with pauses for impact
` : ""}

**Output Format** (valid JSON only):
{
  "scenes": [
    {
      "duration": 12,
      "description": "Wide shot. A sleek Tesla Model 3 drives along Pacific Coast Highway at golden hour. The car's metallic blue paint gleams in warm sunlight. Ocean waves crash against rocky cliffs in the background. Cinematic color grading with teal shadows and orange highlights.",
      "cameraAngle": "wide",
      "movement": "dolly-in",
      "audio": "Ocean waves, wind rushing, electric motor hum",
      "dialogue": null
    },
    {
      "duration": 10,
      "description": "Medium close-up of driver's hands on steering wheel. Modern minimalist interior with wood grain accents. Touchscreen dashboard displaying navigation. Soft natural light from sunroof creates gentle shadows.",
      "cameraAngle": "close-up",
      "movement": "static",
      "audio": "Soft electronic beep from dashboard, leather seats creaking",
      "dialogue": null
    }
  ],
  "voiceoverScript": "Experience the future of driving. Where cutting-edge technology meets timeless design.",
  "music": {
    "mood": "inspiring and modern",
    "genre": "electronic ambient"
  }
}`

    const userPrompt = `Create a cinematic ${input.duration}-second video storyboard for:

**User Brief**: "${input.prompt}"

**Production Specs**:
- Total Duration: ${input.duration} seconds (MUST be exact)
- Scene Count: ${sceneGuidance}
- Aspect Ratio: ${input.aspectRatio}
- Visual Style: ${input.style || "photorealistic"}
${input.voice && input.voice !== "none" ? `- Voiceover: ${input.voice} voice (${Math.floor(input.duration * 2.5)} words approx.)` : "- No voiceover (rely on visuals and ambient audio)"}
${input.referenceImage ? `- Reference Image: Provided (match visual style, color palette, and mood)` : ""}

**Director's Notes**:
✓ Scene durations MUST sum to EXACTLY ${input.duration} seconds
✓ Use professional camera movements (dolly, pan, tracking, etc.) - avoid only "static" shots
✓ Each scene needs cinematic visual descriptions (lighting, composition, color grading)
✓ Include audio descriptions (ambience, sound effects, music cues)
✓ Add dialogue if characters are speaking in the scene
✓ Ensure narrative flow and visual variety between scenes
✓ Leverage the 8-12 second "sweet spot" for Sora 2 video generation
${input.voice && input.voice !== "none" ? `✓ Voiceover script should complement visuals, not just describe them` : ""}

Output ONLY valid JSON (no markdown, no additional text).`

    return { systemPrompt, userPrompt }
  }

  /**
   * Select the best T2I model based on input characteristics
   */
  private selectBestT2IModel(input: UserInput): string {
    // For cartoon/anime styles, use NanoBanana
    if (input.style === "cartoon" || input.style === "anime") {
      return "fal-ai/nanobanana"
    }

    // Default to FLUX-dev for best quality/cost balance
    return DEFAULT_MODELS.t2i
  }

  /**
   * Select the best T2V model based on input characteristics
   */
  private selectBestT2VModel(input: UserInput): string {
    // For short videos (≤12s per scene), prefer Sora 2 for built-in audio and premium quality
    // Sora 2 supports direct T2V with dialogue and lip sync
    if (input.duration <= 12) {
      return "fal-ai/sora-2/text-to-video"
    }

    // For longer videos with multiple scenes, use Sora 2 (each scene ≤12s)
    // This leverages Sora 2's sweet spot while supporting 60s total via multiple scenes
    if (input.duration <= 60) {
      return "fal-ai/sora-2/text-to-video"
    }

    // Fallback to Kling V1 for very long videos or when Sora 2 quota is exceeded
    return "fal-ai/kling-video/v1/standard/image-to-video"
  }

  /**
   * Calculate estimated credits for the workflow
   * Uses shared pricing module to ensure estimate = deduction parity
   */
  private calculateEstimatedCredits(workflow: WorkflowPlan): number {
    const totalDuration = workflow.scenes.reduce((sum, s) => sum + s.duration, 0)

    const estimate = estimateWorkflowCost({
      sceneCount: workflow.scenes.length,
      totalDurationSeconds: totalDuration,
      hasVoiceover: !!workflow.voiceover,
      t2iModel: workflow.recommendedModels.t2i,
      t2vModel: workflow.recommendedModels.t2v,
      ttsModel: workflow.recommendedModels.tts,
    })

    // Round up to nearest 0.5 for user-facing display
    return Math.ceil(estimate * 2) / 2
  }
}

/**
 * Helper function to create AI Planner instance
 */
export function createAIPlanner() {
  return new AIPlanner()
}
